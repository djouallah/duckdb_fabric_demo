name: Download and Update Files

on:
  schedule:
    # Run daily at 7 AM Brisbane time (21:00 UTC)
    - cron: '0 21 * * *'
  workflow_dispatch: # Allows manual trigger

jobs:
  download-files:
    runs-on: ubuntu-latest
    
    steps:
    - name: Minimal checkout (skip existing files)
      run: |
        git clone --depth=1 --filter=blob:none --sparse https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }} .
        git sparse-checkout init --cone
        git sparse-checkout set .github/
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests
    
    - name: Create download script
      run: |
        cat > download_script.py << 'EOF'
        import os
        import re
        import glob
        import requests
        from urllib.request import urlopen
        from collections import defaultdict

        def extract_year_from_filename(filename):
            """Extract year from filename like PUBLIC_DAILY_201804010000_20180402040501.zip"""
            # Look for pattern like PUBLIC_DAILY_YYYYMMDDHHII_
            match = re.search(r'PUBLIC_DAILY_(\d{4})\d{8}_', filename)
            if match:
                return match.group(1)
            return None

        def get_existing_files_by_year(base_path):
            """Get existing files organized by year from remote repository"""
            existing_files = defaultdict(set)
            
            try:
                # Try to get existing files from git ls-tree (much faster than downloading)
                import subprocess
                result = subprocess.run(['git', 'ls-tree', '-r', '--name-only', 'HEAD', base_path], 
                                      capture_output=True, text=True)
                if result.returncode == 0:
                    for filepath in result.stdout.strip().split('\n'):
                        if filepath and filepath.endswith('.zip'):
                            # Extract year from path like data/archive/2018/filename.zip
                            path_parts = filepath.split('/')
                            if len(path_parts) >= 3:  # data/archive/YEAR/file.zip
                                year = path_parts[2]
                                filename = path_parts[-1]
                                existing_files[year].add(filename)
                
                print(f"Found existing files in remote repo: {dict(existing_files)}")
            except Exception as e:
                print(f"Could not check existing files: {e}")
                # If we can't check, assume no existing files (will skip duplicates during download)
            
            return existing_files

        def download_and_organize(url, base_path):
            """Download files and organize them by year"""
            try:
                # Fetch file list from URL
                print(f"Fetching file list from: {url}")
                result = urlopen(url).read().decode('utf-8')
                pattern = re.compile(r'[\w.-]*\.zip')
                all_files = pattern.findall(result)
                
                # Remove duplicates and sort
                unique_files = list(dict.fromkeys(all_files))
                print(f"Found {len(unique_files)} files on server")
                
                # Get existing files organized by year
                existing_files = get_existing_files_by_year(base_path)
                
                # Organize files by year and determine what needs downloading
                files_by_year = defaultdict(list)
                total_new_files = 0
                
                for filename in unique_files:
                    year = extract_year_from_filename(filename)
                    if year:
                        # Check if file already exists in the year directory
                        if filename not in existing_files.get(year, set()):
                            files_by_year[year].append(filename)
                            total_new_files += 1
                        else:
                            print(f"File already exists: {filename} (year: {year})")
                    else:
                        print(f"Could not extract year from filename: {filename}")
                
                print(f"Total new files to download: {total_new_files}")
                
                downloaded_count = 0
                
                # Download files organized by year
                for year, files_to_download in files_by_year.items():
                    if not files_to_download:
                        continue
                        
                    year_path = os.path.join(base_path, year)
                    if not os.path.exists(year_path):
                        os.makedirs(year_path, exist_ok=True)
                        print(f"Created directory: {year_path}")
                    
                    print(f"\nDownloading {len(files_to_download)} files for year {year}:")
                    
                    for filename in files_to_download:
                        try:
                            print(f"  Downloading: {filename}")
                            file_url = url + filename
                            local_path = os.path.join(year_path, filename)
                            
                            with requests.get(file_url, stream=True, timeout=30) as resp:
                                resp.raise_for_status()
                                
                                with open(local_path, "wb") as f:
                                    for chunk in resp.iter_content(chunk_size=8192):
                                        if chunk:
                                            f.write(chunk)
                                
                                print(f"  ✅ Successfully downloaded: {filename}")
                                downloaded_count += 1
                                
                        except Exception as e:
                            print(f"  ❌ Failed to download {filename}: {str(e)}")
                
                print(f"\nDownload summary: {downloaded_count}/{total_new_files} files downloaded successfully")
                return downloaded_count
                
            except Exception as e:
                print(f"❌ Error during download process: {str(e)}")
                return 0

        if __name__ == "__main__":
            base_path = "data/archive"
            url = "https://nemweb.com.au/Reports/Current/Daily_Reports/"
            
            print("Starting download and organization process...")
            print(f"Source URL: {url}")
            print(f"Base path: {base_path}")
            print("-" * 60)
            
            new_files = download_and_organize(url, base_path)
            
            # Set output for next step
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write(f'new_files={new_files}\n')
        EOF
    
    - name: Run download script
      id: download
      run: python download_script.py
    
    - name: Check for changes
      id: git-check
      run: |
        if [ -n "$(git status --porcelain)" ]; then
          echo "changes=true" >> $GITHUB_OUTPUT
        else
          echo "changes=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Commit and push if changes
      if: steps.git-check.outputs.changes == 'true'
      run: |
        # Add new files with sparse option to avoid expanding checkout
        git add --sparse data/archive/
        git commit -m "Auto-update: Downloaded ${{ steps.download.outputs.new_files }} new file(s) organized by year - $(date +'%Y-%m-%d %H:%M:%S UTC')"
        git push
    
    - name: Summary
      run: |
        if [ "${{ steps.git-check.outputs.changes }}" == "true" ]; then
          echo "✅ Downloaded and committed ${{ steps.download.outputs.new_files }} new file(s)"
        else
          echo "ℹ️ No new files found or no changes to commit"
        fi
