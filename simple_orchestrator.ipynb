{"cells":[{"cell_type":"code","source":["!pip install    -q obstore   --upgrade\n","!pip install    -q duckdb    --upgrade\n","import sys\n","sys.exit(0)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"609bd2a6-aabc-477d-ab56-0ddbc987825c"},{"cell_type":"code","source":["try:\n","    import duckdb\n","    duckdb.sql(\" force install delta from core_nightly;\")\n","except:\n","    print(\"all good\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"ba55aad0-df98-4c9f-8813-736411704744"},{"cell_type":"code","source":["ws                    = 'largedata'\n","lh                    = 'simple'\n","schema                = 'test'\n","compaction_threshold  =  150\n","sql_folder            = 'https://github.com/djouallah/fabric_demo/raw/refs/heads/main/transformation/'"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"tags":["parameters"]},"id":"06d4db22-5b31-439b-b326-642378ccc6e6"},{"cell_type":"markdown","source":["<mark>**Core Logic**</mark>"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"213d6177-c428-4d7b-b649-c3df1839d49a"},{"cell_type":"code","source":["import duckdb\n","import requests\n","import os\n","import sys\n","import importlib.util\n","from   deltalake import DeltaTable, write_deltalake\n","from   typing import List, Tuple, Union, Any, Optional, Callable, Dict\n","from   string import Template\n","class Tasksql:\n","    \"\"\"\n","    Simplified Lakehouse task runner supporting:\n","      - ('script_name', (args,))          → runs script_name.py → script_name(*args)\n","      - ('table_name', 'mode', {params})  → runs table_name.sql with params, writes to Delta\n","    \"\"\"\n","\n","    def __init__(self, workspace: str, lakehouse_name: str, schema: str, sql_folder: str, compaction_threshold: int = 10):\n","        self.workspace = workspace\n","        self.lakehouse_name = lakehouse_name\n","        self.schema = schema\n","        self.sql_folder = sql_folder.strip()\n","        self.compaction_threshold = compaction_threshold\n","        self.table_base_url = f'abfss://{self.workspace}@onelake.dfs.fabric.microsoft.com/{self.lakehouse_name}.Lakehouse/Tables/'\n","        self.con = duckdb.connect()\n","        self._attach_lakehouse()\n","\n","    @classmethod\n","    def connect(cls, workspace: str, lakehouse_name: str, schema: str, sql_folder: str, compaction_threshold: int = 10):\n","        print(\"Connecting to Lakehouse...\")\n","        return cls(workspace, lakehouse_name, schema, sql_folder.strip(), compaction_threshold)\n","\n","    def _get_storage_token(self):\n","        return os.environ.get(\"AZURE_STORAGE_TOKEN\", \"PLACEHOLDER_TOKEN_TOKEN_NOT_AVAILABLE\")\n","\n","    def _create_onelake_secret(self):\n","        token = self._get_storage_token()\n","        if token != \"PLACEHOLDER_TOKEN_TOKEN_NOT_AVAILABLE\":\n","            self.con.sql(f\"CREATE OR REPLACE SECRET onelake (TYPE AZURE, PROVIDER ACCESS_TOKEN, ACCESS_TOKEN '{token}')\")\n","        else:\n","            print(\"Please login to Azure CLI\")\n","            self.con.sql(\"CREATE OR REPLACE PERSISTENT SECRET onelake (TYPE azure, PROVIDER credential_chain, CHAIN 'cli', ACCOUNT_NAME 'onelake')\")\n","\n","    def _attach_lakehouse(self):\n","        self._create_onelake_secret()\n","        try:\n","            list_tables_query = f\"\"\"\n","                SELECT DISTINCT(split_part(file, '_delta_log', 1)) as tables\n","                FROM glob (\"abfss://{self.workspace}@onelake.dfs.fabric.microsoft.com/{self.lakehouse_name}.Lakehouse/Tables/*/*/_delta_log/*.json\")\n","            \"\"\"\n","            list_tables_df = self.con.sql(list_tables_query).df()\n","            list_tables = list_tables_df['tables'].tolist() if not list_tables_df.empty else []\n","\n","            if not list_tables:\n","                print(f\"No Delta tables found in {self.lakehouse_name}.Lakehouse/Tables.\")\n","                return\n","\n","            print(f\"Found {len(list_tables)} Delta tables. Attaching as views...\")\n","\n","            for table_path in list_tables:\n","                parts = table_path.strip(\"/\").split(\"/\")\n","                if len(parts) >= 2:\n","                    potential_schema = parts[-2]\n","                    table = parts[-1]\n","                    if potential_schema == self.schema:\n","                        try:\n","                            self.con.sql(f\"\"\"\n","                                CREATE OR REPLACE VIEW {table}\n","                                AS SELECT * FROM delta_scan('{self.table_base_url}{self.schema}/{table}');\n","                            \"\"\")\n","                        except Exception as e:\n","                            print(f\"Error creating view for table {table}: {e}\")\n","            print(\"\\nAttached tables (views) in DuckDB:\")\n","            self.con.sql(\"SELECT name FROM (SHOW ALL TABLES) WHERE database='memory'\").show()\n","        except Exception as e:\n","            print(f\"Error attaching lakehouse: {e}\")\n","\n","    def _normalize_table_name(self, name: str) -> str:\n","        \"\"\"\n","        Extract base table name by taking the part before the first double underscore '__'.\n","        If no underscore, return the name as-is.\n","        \n","        Examples:\n","            'sales__update' → 'sales'\n","            'fact__daily_v2' → 'fact'\n","            'events' → 'events'\n","        \"\"\"\n","        if '__' in name:\n","            return name.split('__', 1)[0]\n","        return name\n","\n","    def _read_sql_file(self, table_name: str, params: Optional[Dict] = None) -> Optional[str]:\n","        is_url = self.sql_folder.startswith(\"http\")\n","        if is_url:\n","            url = f\"{self.sql_folder.rstrip('/')}/{table_name}.sql\".strip()\n","            try:\n","                resp = requests.get(url)\n","                resp.raise_for_status()\n","                content = resp.text\n","            except Exception as e:\n","                print(f\"Failed to fetch SQL from {url}: {e}\")\n","                return None\n","        else:\n","            path = os.path.join(self.sql_folder, f\"{table_name}.sql\")\n","            try:\n","                with open(path, 'r') as f:\n","                    content = f.read()\n","            except Exception as e:\n","                print(f\"Failed to read SQL file {path}: {e}\")\n","                return None\n","\n","        if not content.strip():\n","            print(f\"SQL file is empty: {table_name}.sql\")\n","            return None\n","\n","        # Merge system + user params\n","        full_params = {\n","            'ws': self.workspace,\n","            'lh': self.lakehouse_name,\n","            'schema': self.schema\n","        }\n","        if params:\n","            full_params.update(params)\n","\n","        # Use string.Template ($ws, ${run_date}) — safe with DuckDB {}\n","        try:\n","            template = Template(content)\n","            content = template.substitute(full_params)\n","        except KeyError as e:\n","            print(f\"Missing parameter in SQL file: ${e}\")\n","            return None\n","        except Exception as e:\n","            print(f\"Error during SQL template substitution: {e}\")\n","            return None\n","\n","        return content\n","\n","    def _load_py_function(self, name: str) -> Optional[Callable]:\n","        is_url = self.sql_folder.startswith(\"http\")\n","        try:\n","            if is_url:\n","                url = f\"{self.sql_folder.rstrip('/')}/{name}.py\".strip()\n","                resp = requests.get(url)\n","                resp.raise_for_status()\n","                code = resp.text\n","                namespace = {}\n","                exec(code, namespace)\n","                func = namespace.get(name)\n","                return func if callable(func) else None\n","            else:\n","                path = os.path.join(self.sql_folder, f\"{name}.py\")\n","                if not os.path.isfile(path):\n","                    print(f\"Python file not found: {path}\")\n","                    return None\n","                spec = importlib.util.spec_from_file_location(name, path)\n","                mod = importlib.util.module_from_spec(spec)\n","                spec.loader.exec_module(mod)\n","                func = getattr(mod, name, None)\n","                return func if callable(func) else None\n","        except Exception as e:\n","            print(f\"Error loading Python function '{name}': {e}\")\n","            return None\n","\n","    def _run_py_task(self, name: str, args: tuple) -> int:\n","        func = self._load_py_function(name)\n","        if not func:\n","            return 0\n","        try:\n","            print(f\"Running Python task: {name}{args}\")\n","            result = func(*args)\n","            print(f\"✅ Python task '{name}' completed.\")\n","            return result\n","        except Exception as e:\n","            print(f\"❌ Error in Python task '{name}': {e}\")\n","            return 0\n","\n","    def _run_sql_task(self, table: str, mode: str, params: Optional[Dict] = None) -> int:\n","        allowed_modes = {'overwrite', 'append', 'ignore'}\n","        if mode not in allowed_modes:\n","            print(f\"Invalid mode '{mode}'. Use: {allowed_modes}\")\n","            return 0\n","\n","        sql = self._read_sql_file(table, params)  # loads table.sql or table_anything.sql as-is\n","        if sql is None:\n","            return 0\n","\n","        normalized_table = self._normalize_table_name(table)\n","        path = f\"{self.table_base_url}{self.schema}/{normalized_table}\"\n","\n","        try:\n","            if mode == 'overwrite':\n","                self.con.sql(f\"DROP VIEW IF EXISTS {normalized_table}\")\n","                df = self.con.sql(sql).arrow()\n","                write_deltalake(\n","                    path, df, mode='overwrite',\n","                    max_rows_per_file=8_000_000,\n","                    max_rows_per_group=8_000_000,\n","                    min_rows_per_group=8_000_000,\n","                    engine='pyarrow'\n","                )\n","                self.con.sql(f\"CREATE OR REPLACE VIEW {normalized_table} AS SELECT * FROM delta_scan('{path}')\")\n","                dt = DeltaTable(path)\n","                dt.vacuum(retention_hours=0, dry_run=False, enforce_retention_duration=False)\n","                dt.cleanup_metadata()\n","\n","            elif mode == 'append':\n","                df = self.con.sql(sql).arrow()\n","                write_deltalake(\n","                    path, df, mode='append',\n","                    max_rows_per_file=8_000_000,\n","                    max_rows_per_group=8_000_000,\n","                    min_rows_per_group=8_000_000,\n","                    engine='pyarrow'\n","                )\n","                self.con.sql(f\"CREATE OR REPLACE VIEW {normalized_table} AS SELECT * FROM delta_scan('{path}')\")\n","                dt = DeltaTable(path)\n","                if len(dt.files()) > self.compaction_threshold:\n","                    print(f\"Compacting {normalized_table} (files: {len(dt.files())})\")\n","                    dt.optimize.compact()\n","                    dt.vacuum(dry_run=False)\n","                    dt.cleanup_metadata()\n","\n","            elif mode == 'ignore':\n","                try:\n","                    DeltaTable(path)\n","                    print(f\"Table {normalized_table} exists. Skipping (mode='ignore').\")\n","                except Exception:\n","                    print(f\"{normalized_table} doesn't exist. Creating in overwrite mode.\")\n","                    self.con.sql(f\"DROP VIEW IF EXISTS {normalized_table}\")\n","                    df = self.con.sql(sql).arrow()\n","                    write_deltalake(\n","                        path, df, mode='overwrite',\n","                        max_rows_per_file=8_000_000,\n","                        max_rows_per_group=8_000_000,\n","                        min_rows_per_group=8_000_000,\n","                        engine='pyarrow'\n","                    )\n","                    self.con.sql(f\"CREATE OR REPLACE VIEW {normalized_table} AS SELECT * FROM delta_scan('{path}')\")\n","                    dt = DeltaTable(path)\n","                    dt.vacuum(dry_run=False)\n","                    dt.cleanup_metadata()\n","\n","            print(f\"✅ SQL task '{table}' → table '{normalized_table}' ({mode}) completed.\")\n","            return 1\n","\n","        except Exception as e:\n","            print(f\"❌ Error in SQL task '{table}' (writing to '{normalized_table}'): {e}\")\n","            return 0\n","\n","    def run_pipeline(self, tasks: List[Union[Tuple[str, tuple], Tuple[str, str, Dict]]]) -> bool:\n","        \"\"\"\n","        Run tasks with simple syntax:\n","          - ('download', (url_list, path_list, depth))\n","          - ('staging_table', 'overwrite', {'run_date': '2024-06-01'})\n","        \"\"\"\n","        for i, task in enumerate(tasks):\n","            print(f\"\\n--- Running Task {i+1}: {task[0]} ---\")\n","            name = task[0]\n","\n","            if len(task) == 2:\n","                # Python task: ('name', (args,))\n","                args = task[1]\n","                if not isinstance(args, (tuple, list)):\n","                    args = (args,)\n","                result = self._run_py_task(name, tuple(args))\n","            elif len(task) == 3:\n","                # SQL write task: ('table', 'mode', {params})\n","                mode, params = task[1], task[2]\n","                if not isinstance(params, dict):\n","                    print(f\"❌ Expected dict as 3rd item in SQL task, got {type(params)}\")\n","                    return False\n","                result = self._run_sql_task(name, mode, params)\n","            else:\n","                print(f\"❌ Invalid task format: {task}\")\n","                return False\n","\n","            if result != 1:\n","                print(f\"❌ Task {i+1} failed. Stopping.\")\n","                return False\n","\n","        print(\"\\n✅ All tasks completed successfully.\")\n","        return True\n","\n","    def get_connection(self):\n","        return self.con\n","\n","    def close(self):\n","        if self.con:\n","            self.con.close()\n","            print(\"DuckDB connection closed.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"jupyter":{"source_hidden":true}},"id":"02e7908e-78cb-4a78-92fa-7e40d60a7185"},{"cell_type":"code","source":["%%time\n","con = Tasksql.connect( ws,lh,schema, sql_folder, compaction_threshold  )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"f02dab12-9c27-4be1-b4f4-4fca2d598dd0"},{"cell_type":"code","source":["nightly =[\n","              ('scraping', ([\"https://nemweb.com.au/Reports/Current/Daily_Reports/\"],[\"Reports/Current/Daily_Reports/\"],60,ws,lh,6)),\n","              ('price','append',{'ws': ws,'lh':lh}),\n","              ('scada','append',{'ws': ws,'lh':lh}),\n","              ('download_excel',(\"raw/\", ws,lh)),\n","              ('duid','overwrite',{'ws': ws,'lh':lh}),\n","              ('calendar','ignore',{}),\n","              ('mstdatetime','ignore',{}),\n","              ('summary__backfill','overwrite',{})\n","                     ]\n","\n","intraday = [\n","              ('scraping', ([\"http://nemweb.com.au/Reports/Current/DispatchIS_Reports/\", \"http://nemweb.com.au/Reports/Current/Dispatch_SCADA/\" ],\n","                            [\"Reports/Current/DispatchIS_Reports/\",\"Reports/Current/Dispatch_SCADA/\"],\n","                             288, ws,lh,6)),\n","              ('price_today','append',{'ws': ws,'lh':lh}),\n","              ('scada_today','append',{'ws': ws,'lh':lh}),\n","              ('duid','ignore',{'ws': ws,'lh':lh}),\n","              ('summary__incremental', 'append',{})            \n","                    ]\n","history_download = [\n","               ('scraping',\n","                          ([\"https://github.com/djouallah/fabric_demo/tree/main/data/archive/2025\",\n","                            \"https://github.com/djouallah/fabric_demo/tree/main/data/archive/2024\",\n","                            \"https://github.com/djouallah/fabric_demo/tree/main/data/archive/2023\",\n","                            \"https://github.com/djouallah/fabric_demo/tree/main/data/archive/2022\",\n","                            \"https://github.com/djouallah/fabric_demo/tree/main/data/archive/2021\",\n","                            \"https://github.com/djouallah/fabric_demo/tree/main/data/archive/2020\",\n","                            \"https://github.com/djouallah/fabric_demo/tree/main/data/archive/2019\",\n","                            \"https://github.com/djouallah/fabric_demo/tree/main/data/archive/2018\"\n","                          ],\n","                         [\"Reports/Current/Daily_Reports/\",\n","                          \"Reports/Current/Daily_Reports/\",\n","                          \"Reports/Current/Daily_Reports/\",\n","                          \"Reports/Current/Daily_Reports/\",\n","                          \"Reports/Current/Daily_Reports/\",\n","                          \"Reports/Current/Daily_Reports/\",\n","                          \"Reports/Current/Daily_Reports/\",\n","                          \"Reports/Current/Daily_Reports/\"\n","                         ]\n","                         ,7,ws,lh,6))\n","                      ]\n","history_process = [\n","              ('price','append',{'ws': ws,'lh':lh}),\n","              ('scada','append',{'ws': ws,'lh':lh}),\n","              ('summary__backfill_archive','append',{})\n","                   ]"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"0cb55004-4897-450e-ae9d-42266a576085"},{"cell_type":"code","source":["%%time\n","con.run_pipeline(nightly)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"3348a4c5-068e-4300-a046-dd1f1da97b28"},{"cell_type":"code","source":["%%time\n","con.run_pipeline(intraday)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"3eed16b6-c7e1-4fb7-b85a-e7fb40c4ccf5"},{"cell_type":"code","source":["%%time\n","## you can turn it off when Historical data is fully loaded\n","con.run_pipeline(history_download)\n","con.run_pipeline(history_process)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"a62bd1bb-42c1-46a7-8e02-7bb1b9125b98"}],"metadata":{"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"name":"jupyter","language":"Jupyter","display_name":"Jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"720000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}