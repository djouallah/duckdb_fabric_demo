{"cells":[{"cell_type":"markdown","source":["**_<mark> you can use more cores to make backfill faster : 2,4,8,16,32,64</mark>_**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"180e3d93-20c5-4c24-aedd-b7fdca6cb12d"},{"cell_type":"code","source":["#%%configure\n","#{\"vCores\": 2}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6130918Z","session_start_time":"2025-10-02T11:35:30.613978Z","execution_start_time":"2025-10-02T11:35:35.5017314Z","execution_finish_time":"2025-10-02T11:35:36.0308727Z","parent_msg_id":"52d412a6-1aeb-4c0f-be36-e6a659bcb4f8"}},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"86b4d7de-0c1c-4a25-a1ea-6d3e2fb980b7"},{"cell_type":"code","source":["!pip install    obstore       --upgrade"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6170206Z","session_start_time":null,"execution_start_time":"2025-10-02T11:35:36.0319434Z","execution_finish_time":"2025-10-02T11:35:41.0088762Z","parent_msg_id":"3db20aa3-7959-476b-9e6a-727f314e85c7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting obstore\n  Downloading obstore-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (840 bytes)\nRequirement already satisfied: typing-extensions in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from obstore) (4.14.0)\nDownloading obstore-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: obstore\nSuccessfully installed obstore-0.8.2\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"609bd2a6-aabc-477d-ab56-0ddbc987825c"},{"cell_type":"code","source":["try:\n","    import duckdb\n","    from notebookutils.common import configs\n","    configs.tokenCacheEnabled = False\n","    duckdb.sql(\" force install delta from core_nightly;\")\n","    duckdb.sql(\"update extensions\").show()\n","except:\n","    print(\"all good\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6191197Z","session_start_time":null,"execution_start_time":"2025-10-02T11:35:41.009966Z","execution_finish_time":"2025-10-02T11:35:42.5543814Z","parent_msg_id":"31887da9-2d50-43af-b247-1b1a2532a3dc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["┌────────────────┬──────────────┬─────────────────────┬──────────────────┬─────────────────┐\n│ extension_name │  repository  │    update_result    │ previous_version │ current_version │\n│    varchar     │   varchar    │       varchar       │     varchar      │     varchar     │\n├────────────────┼──────────────┼─────────────────────┼──────────────────┼─────────────────┤\n│ azure          │ core         │ NO_UPDATE_AVAILABLE │ 1593cb5          │ 1593cb5         │\n│ delta          │ core_nightly │ NO_UPDATE_AVAILABLE │ b60cf00          │ b60cf00         │\n└────────────────┴──────────────┴─────────────────────┴──────────────────┴─────────────────┘\n\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"ba55aad0-df98-4c9f-8813-736411704744"},{"cell_type":"code","source":["from   psutil import *\n","core              = cpu_count()\n","Nbr_threads       = (core*2)+1"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6213055Z","session_start_time":null,"execution_start_time":"2025-10-02T11:35:42.5556248Z","execution_finish_time":"2025-10-02T11:35:42.9753065Z","parent_msg_id":"8b81f4de-bb89-456b-a906-b9539cb0d78c"}},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"6ca729fe-a2c7-4c6e-8ea1-92212883b5d7"},{"cell_type":"code","source":["# please don't use a workspace name, Lakehouse and semantic_model with an empty space, or the same name of the lakehouse recently deleted\n","nbr_days_download     =  int(30 * 2 ** ((core - 2) / 2))  # or just input your numbers\n","lh                    = 'power' \n","schema                = 'aemo'\n","semantic_model        = \"directlake_on_onelake\" \n","ws                    =  notebookutils.runtime.context.get(\"currentWorkspaceName\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6234601Z","session_start_time":null,"execution_start_time":"2025-10-02T11:35:42.9765552Z","execution_finish_time":"2025-10-02T11:35:46.1334603Z","parent_msg_id":"ee5f1fae-c5fd-452b-97fa-d5f80f7d618f"}},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"tags":["parameters"]},"id":"94d56bdf-6445-471e-8703-946039ae4bf7"},{"cell_type":"code","source":["compaction_threshold  =  150\n","sql_folder            = 'https://github.com/djouallah/fabric_demo/raw/refs/heads/main/transformation/'\n","directlake_model      = \"https://raw.githubusercontent.com/djouallah/fabric_demo/refs/heads/main/semantic_model/directlake.bim\"\n","remaining_files       = max(0,nbr_days_download - 60)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6254171Z","session_start_time":null,"execution_start_time":"2025-10-02T11:35:46.137003Z","execution_finish_time":"2025-10-02T11:35:46.5554955Z","parent_msg_id":"8751b93e-5486-404b-a415-2aaae39c2b7e"}},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"tags":[]},"id":"06d4db22-5b31-439b-b326-642378ccc6e6"},{"cell_type":"markdown","source":["<mark>**Core Logic**</mark>"],"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"nteract":{"transient":{"deleting":false}}},"id":"213d6177-c428-4d7b-b649-c3df1839d49a"},{"cell_type":"code","source":["import duckdb\n","import requests\n","import os\n","import sys\n","import importlib.util\n","from   deltalake import DeltaTable, write_deltalake\n","from   typing import List, Tuple, Union, Any, Optional, Callable, Dict\n","from   string import Template\n","class Tasksql:\n","    \"\"\"\n","    Simplified Lakehouse task runner supporting:\n","      - ('script_name', (args,))          → runs script_name.py → script_name(*args)\n","      - ('table_name', 'mode', {params})  → runs table_name.sql with params, writes to Delta\n","    \"\"\"\n","\n","    def __init__(self, workspace: str, lakehouse_name: str, schema: str, sql_folder: str, compaction_threshold: int = 10):\n","        self.workspace = workspace\n","        self.lakehouse_name = lakehouse_name\n","        self.schema = schema\n","        self.sql_folder = sql_folder.strip()\n","        self.compaction_threshold = compaction_threshold\n","        self.table_base_url = f'abfss://{self.workspace}@onelake.dfs.fabric.microsoft.com/{self.lakehouse_name}.Lakehouse/Tables/'\n","        self.con = duckdb.connect()\n","        self.con.sql(\"SET preserve_insertion_order = false\")\n","        self._attach_lakehouse()\n","\n","    @classmethod\n","    def connect(cls, workspace: str, lakehouse_name: str, schema: str, sql_folder: str, compaction_threshold: int = 10):\n","        print(\"Connecting to Lakehouse...\")\n","        return cls(workspace, lakehouse_name, schema, sql_folder.strip(), compaction_threshold)\n","\n","    def _get_storage_token(self):\n","        return os.environ.get(\"AZURE_STORAGE_TOKEN\", \"PLACEHOLDER_TOKEN_TOKEN_NOT_AVAILABLE\")\n","\n","    def _create_onelake_secret(self):\n","        token = self._get_storage_token()\n","        if token != \"PLACEHOLDER_TOKEN_TOKEN_NOT_AVAILABLE\":\n","            self.con.sql(f\"CREATE OR REPLACE SECRET onelake (TYPE AZURE, PROVIDER ACCESS_TOKEN, ACCESS_TOKEN '{token}')\")\n","        else:\n","            print(\"Please login to Azure CLI\")\n","            from azure.identity import AzureCliCredential, InteractiveBrowserCredential, ChainedTokenCredential\n","            credential = ChainedTokenCredential( AzureCliCredential(), InteractiveBrowserCredential())\n","            token = credential.get_token(\"https://storage.azure.com/.default\")\n","            os.environ[\"AZURE_STORAGE_TOKEN\"] = token.token\n","            self.con.sql(\"CREATE OR REPLACE PERSISTENT SECRET onelake (TYPE azure, PROVIDER credential_chain, CHAIN 'cli', ACCOUNT_NAME 'onelake')\")\n","\n","    def _attach_lakehouse(self):\n","        self._create_onelake_secret()\n","        try:\n","            list_tables_query = f\"\"\"\n","                SELECT DISTINCT(split_part(file, '_delta_log', 1)) as tables\n","                FROM glob (\"abfss://{self.workspace}@onelake.dfs.fabric.microsoft.com/{self.lakehouse_name}.Lakehouse/Tables/*/*/_delta_log/*.json\")\n","            \"\"\"\n","            list_tables_df = self.con.sql(list_tables_query).df()\n","            list_tables = list_tables_df['tables'].tolist() if not list_tables_df.empty else []\n","\n","            if not list_tables:\n","                print(f\"No Delta tables found in {self.lakehouse_name}.Lakehouse/Tables.\")\n","                return\n","\n","            print(f\"Found {len(list_tables)} Delta tables. Attaching as views...\")\n","\n","            for table_path in list_tables:\n","                parts = table_path.strip(\"/\").split(\"/\")\n","                if len(parts) >= 2:\n","                    potential_schema = parts[-2]\n","                    table = parts[-1]\n","                    if potential_schema == self.schema:\n","                        try:\n","                            self.con.sql(f\"\"\"\n","                                CREATE OR REPLACE VIEW {table}\n","                                AS SELECT * FROM delta_scan('{self.table_base_url}{self.schema}/{table}');\n","                            \"\"\")\n","                        except Exception as e:\n","                            print(f\"Error creating view for table {table}: {e}\")\n","            print(\"\\nAttached tables (views) in DuckDB:\")\n","            self.con.sql(\"SELECT name FROM (SHOW ALL TABLES) WHERE database='memory'\").show()\n","        except Exception as e:\n","            print(f\"Error attaching lakehouse: {e}\")\n","\n","    def _normalize_table_name(self, name: str) -> str:\n","        \"\"\"\n","        Extract base table name by taking the part before the first double underscore '__'.\n","        If no underscore, return the name as-is.\n","        \n","        Examples:\n","            'sales__update' → 'sales'\n","            'fact__daily_v2' → 'fact'\n","            'events' → 'events'\n","        \"\"\"\n","        if '__' in name:\n","            return name.split('__', 1)[0]\n","        return name\n","\n","    def _read_sql_file(self, table_name: str, params: Optional[Dict] = None) -> Optional[str]:\n","        is_url = self.sql_folder.startswith(\"http\")\n","        if is_url:\n","            url = f\"{self.sql_folder.rstrip('/')}/{table_name}.sql\".strip()\n","            try:\n","                resp = requests.get(url)\n","                resp.raise_for_status()\n","                content = resp.text\n","            except Exception as e:\n","                print(f\"Failed to fetch SQL from {url}: {e}\")\n","                return None\n","        else:\n","            path = os.path.join(self.sql_folder, f\"{table_name}.sql\")\n","            try:\n","                with open(path, 'r') as f:\n","                    content = f.read()\n","            except Exception as e:\n","                print(f\"Failed to read SQL file {path}: {e}\")\n","                return None\n","\n","        if not content.strip():\n","            print(f\"SQL file is empty: {table_name}.sql\")\n","            return None\n","\n","        # Merge system + user params\n","        full_params = {\n","            'ws': self.workspace,\n","            'lh': self.lakehouse_name,\n","            'schema': self.schema\n","        }\n","        if params:\n","            full_params.update(params)\n","\n","        # Use string.Template ($ws, ${run_date}) — safe with DuckDB {}\n","        try:\n","            template = Template(content)\n","            content = template.substitute(full_params)\n","        except KeyError as e:\n","            print(f\"Missing parameter in SQL file: ${e}\")\n","            return None\n","        except Exception as e:\n","            print(f\"Error during SQL template substitution: {e}\")\n","            return None\n","\n","        return content\n","\n","    def _load_py_function(self, name: str) -> Optional[Callable]:\n","        is_url = self.sql_folder.startswith(\"http\")\n","        try:\n","            if is_url:\n","                url = f\"{self.sql_folder.rstrip('/')}/{name}.py\".strip()\n","                resp = requests.get(url)\n","                resp.raise_for_status()\n","                code = resp.text\n","                namespace = {}\n","                exec(code, namespace)\n","                func = namespace.get(name)\n","                return func if callable(func) else None\n","            else:\n","                path = os.path.join(self.sql_folder, f\"{name}.py\")\n","                if not os.path.isfile(path):\n","                    print(f\"Python file not found: {path}\")\n","                    return None\n","                spec = importlib.util.spec_from_file_location(name, path)\n","                mod = importlib.util.module_from_spec(spec)\n","                spec.loader.exec_module(mod)\n","                func = getattr(mod, name, None)\n","                return func if callable(func) else None\n","        except Exception as e:\n","            print(f\"Error loading Python function '{name}': {e}\")\n","            return None\n","\n","    def _run_py_task(self, name: str, args: tuple) -> int:\n","        self._create_onelake_secret()\n","        func = self._load_py_function(name)\n","        if not func:\n","            return 0\n","        try:\n","            print(f\"Running Python task: {name}{args}\")\n","            result = func(*args)\n","            print(f\"✅ Python task '{name}' completed.\")\n","            return result\n","        except Exception as e:\n","            print(f\"❌ Error in Python task '{name}': {e}\")\n","            return 0\n","\n","    def _run_sql_task(self, table: str, mode: str, params: Optional[Dict] = None) -> int:\n","        self._create_onelake_secret()\n","        allowed_modes = {'overwrite', 'append', 'ignore'}\n","        if mode not in allowed_modes:\n","            print(f\"Invalid mode '{mode}'. Use: {allowed_modes}\")\n","            return 0\n","\n","        sql = self._read_sql_file(table, params)  # loads table.sql or table_anything.sql as-is\n","        if sql is None:\n","            return 0\n","\n","        normalized_table = self._normalize_table_name(table)\n","        path = f\"{self.table_base_url}{self.schema}/{normalized_table}\"\n","\n","        try:\n","            if mode == 'overwrite':\n","                self.con.sql(f\"DROP VIEW IF EXISTS {normalized_table}\")\n","                df = self.con.sql(sql).record_batch()\n","                write_deltalake(path, df, mode='overwrite')\n","                self.con.sql(f\"CREATE OR REPLACE VIEW {normalized_table} AS SELECT * FROM delta_scan('{path}')\")\n","                dt = DeltaTable(path)\n","                dt.vacuum(retention_hours=0, dry_run=False, enforce_retention_duration=False)\n","                dt.cleanup_metadata()\n","\n","            elif mode == 'append':\n","                df = self.con.sql(sql).record_batch()\n","                write_deltalake(path, df, mode='append')\n","                self.con.sql(f\"CREATE OR REPLACE VIEW {normalized_table} AS SELECT * FROM delta_scan('{path}')\")\n","                dt = DeltaTable(path)\n","                if len(dt.file_uris()) > self.compaction_threshold:\n","                    print(f\"Compacting {normalized_table} (files: {len(dt.file_uris())})\")\n","                    dt.optimize.compact()\n","                    dt.vacuum(dry_run=False)\n","                    dt.cleanup_metadata()\n","\n","            elif mode == 'ignore':\n","                try:\n","                    DeltaTable(path)\n","                    print(f\"Table {normalized_table} exists. Skipping (mode='ignore').\")\n","                except Exception:\n","                    print(f\"{normalized_table} doesn't exist. Creating in overwrite mode.\")\n","                    self.con.sql(f\"DROP VIEW IF EXISTS {normalized_table}\")\n","                    df = self.con.sql(sql).record_batch()\n","                    write_deltalake(path, df, mode='overwrite')\n","                    self.con.sql(f\"CREATE OR REPLACE VIEW {normalized_table} AS SELECT * FROM delta_scan('{path}')\")\n","                    dt = DeltaTable(path)\n","                    dt.vacuum(dry_run=False)\n","                    dt.cleanup_metadata()\n","\n","            print(f\"✅ SQL task '{table}' → table '{normalized_table}' ({mode}) completed.\")\n","            return 1\n","\n","        except Exception as e:\n","            print(f\"❌ Error in SQL task '{table}' (writing to '{normalized_table}'): {e}\")\n","            return 0\n","\n","    def run_pipeline(self, tasks: List[Union[Tuple[str, tuple], Tuple[str, str, Dict]]]) -> bool:\n","        \"\"\"\n","        Run tasks with simple syntax:\n","          - ('download', (url_list, path_list, depth))\n","          - ('staging_table', 'overwrite', {'run_date': '2024-06-01'})\n","        \"\"\"\n","        for i, task in enumerate(tasks):\n","            print(f\"\\n--- Running Task {i+1}: {task[0]} ---\")\n","            name = task[0]\n","\n","            if len(task) == 2:\n","                # Python task: ('name', (args,))\n","                args = task[1]\n","                if not isinstance(args, (tuple, list)):\n","                    args = (args,)\n","                result = self._run_py_task(name, tuple(args))\n","            elif len(task) == 3:\n","                # SQL write task: ('table', 'mode', {params})\n","                mode, params = task[1], task[2]\n","                if not isinstance(params, dict):\n","                    print(f\"❌ Expected dict as 3rd item in SQL task, got {type(params)}\")\n","                    return False\n","                result = self._run_sql_task(name, mode, params)\n","            else:\n","                print(f\"❌ Invalid task format: {task}\")\n","                return False\n","\n","            if result != 1:\n","                print(f\"❌ Task {i+1} failed. Stopping.\")\n","                return False\n","\n","        print(\"\\n✅ All tasks completed successfully.\")\n","        return True\n","\n","    def get_connection(self):\n","        return self.con\n","    \n","    def sql(self, query: str):\n","        \"\"\"\n","        Execute an arbitrary SQL query on the DuckDB connection.\n","        Returns the result which can be converted to various formats (.df(), .show(), etc.)\n","        \n","        Example:\n","            ts.sql(\"SELECT * FROM my_table LIMIT 10\").show()\n","            df = ts.sql(\"SELECT COUNT(*) FROM my_table\").df()\n","        \"\"\"\n","        return self.con.sql(query)\n","\n","    def close(self):\n","        if self.con:\n","            self.con.close()\n","            print(\"DuckDB connection closed.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6275279Z","session_start_time":null,"execution_start_time":"2025-10-02T11:35:46.5568022Z","execution_finish_time":"2025-10-02T11:35:49.6768183Z","parent_msg_id":"9950ed7b-8da0-40e9-8323-81cc44140c7e"}},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":true},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"02e7908e-78cb-4a78-92fa-7e40d60a7185"},{"cell_type":"code","source":["%%time\n","con = Tasksql.connect( ws,lh,schema, sql_folder, compaction_threshold  )"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6320264Z","session_start_time":null,"execution_start_time":"2025-10-02T11:35:49.678123Z","execution_finish_time":"2025-10-02T11:35:54.0781275Z","parent_msg_id":"c337bdcc-d9a6-478a-b4f0-3f727862626d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Connecting to Lakehouse...\nFound 9 Delta tables. Attaching as views...\n\nAttached tables (views) in DuckDB:\n┌─────────────┐\n│    name     │\n│   varchar   │\n├─────────────┤\n│ calendar    │\n│ duid        │\n│ mstdatetime │\n│ price       │\n│ price_today │\n│ scada       │\n│ scada_today │\n│ summary     │\n└─────────────┘\n\nCPU times: user 1.22 s, sys: 58.2 ms, total: 1.27 s\nWall time: 4.14 s\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"f02dab12-9c27-4be1-b4f4-4fca2d598dd0"},{"cell_type":"code","source":["nightly =[\n","              \n","              ('scrapingv2', ([\"https://nemweb.com.au/Reports/Current/Daily_Reports/\"],[\"Reports/Current/Daily_Reports/\"],nbr_days_download,ws,lh,Nbr_threads)),\n","              ('price','append',{'ws': ws,'lh':lh}),\n","              ('scada','append',{'ws': ws,'lh':lh}),\n","              ('download_excel',(\"raw/\", ws,lh)),\n","              ('duid','overwrite',{'ws': ws,'lh':lh}),\n","              ('calendar','ignore',{}),\n","              ('mstdatetime','ignore',{}),\n","              ('summary__backfill','overwrite',{})\n","         ]\n","\n","intraday = [\n","              ('scrapingv2', ([\"http://nemweb.com.au/Reports/Current/DispatchIS_Reports/\",\"http://nemweb.com.au/Reports/Current/Dispatch_SCADA/\" ],\n","                            [\"Reports/Current/DispatchIS_Reports/\",\"Reports/Current/Dispatch_SCADA/\"],\n","                             288, ws,lh,Nbr_threads)),\n","              ('price_today','append',{'ws': ws,'lh':lh}),\n","              ('scada_today','append',{'ws': ws,'lh':lh}),\n","              ('duid','ignore',{'ws': ws,'lh':lh}),\n","              ('summary__incremental', 'append',{})            \n","          ]\n","\n","history_download = [('scrapingv2',([\"https://github.com/djouallah/fabric_demo/tree/main/data/archive/*\"],[\"Reports/Current/Daily_Reports/\"],remaining_files,ws,lh,Nbr_threads))]\n","\n","history_process = [('scada','append',{'ws': ws,'lh':lh}),('price','append',{'ws': ws,'lh':lh}),('summary__backfill_archive','append',{})]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6385011Z","session_start_time":null,"execution_start_time":"2025-10-02T11:35:54.0793385Z","execution_finish_time":"2025-10-02T11:35:54.5233729Z","parent_msg_id":"069d7fe3-1c51-4242-9d17-ebbefb5268a5"}},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"jupyter":{"source_hidden":false}},"id":"0cb55004-4897-450e-ae9d-42266a576085"},{"cell_type":"code","source":["%%time\n","#create lakehouse if not exists\n","con.run_pipeline([('create_lakehouse_if_not_exists', (lh))])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6405065Z","session_start_time":null,"execution_start_time":"2025-10-02T11:35:54.5245925Z","execution_finish_time":"2025-10-02T11:35:55.9613593Z","parent_msg_id":"059a45c4-bc1b-4bd3-bc53-06ec419bab66"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n--- Running Task 1: create_lakehouse_if_not_exists ---\nRunning Python task: create_lakehouse_if_not_exists('power',)\n✅ Python task 'create_lakehouse_if_not_exists' completed.\n\n✅ All tasks completed successfully.\nCPU times: user 91.1 ms, sys: 4.83 ms, total: 96 ms\nWall time: 1.08 s\n"]},{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"True"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"84c588ea-e9ff-4656-ab7d-b4bf656d3fbd"},{"cell_type":"code","source":["%%time\n","#initial load\n","con.run_pipeline(nightly)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6424594Z","session_start_time":null,"execution_start_time":"2025-10-02T11:35:55.9626964Z","execution_finish_time":"2025-10-02T11:35:57.3724048Z","parent_msg_id":"3b61a29e-b9ba-4664-8e9d-1a78951c8e62"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n--- Running Task 1: scrapingv2 ---\nRunning Python task: scrapingv2(['https://nemweb.com.au/Reports/Current/Daily_Reports/'], ['Reports/Current/Daily_Reports/'], 30, 'temp', 'power', 5)\nhttps://nemweb.com.au/Reports/Current/Daily_Reports/ - 0 files extracted (all 60 files already downloaded)\n✅ Python task 'scrapingv2' completed.\n❌ Task 1 failed. Stopping.\nCPU times: user 96.8 ms, sys: 7 ms, total: 104 ms\nWall time: 1.24 s\n"]},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"False"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"3348a4c5-068e-4300-a046-dd1f1da97b28"},{"cell_type":"code","source":["%%time\n","#create  semantic model\n","con.run_pipeline([('deploy_modelv2', (lh,schema,semantic_model,directlake_model))])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.644354Z","session_start_time":null,"execution_start_time":"2025-10-02T11:35:57.3736797Z","execution_finish_time":"2025-10-02T11:36:12.8823643Z","parent_msg_id":"29e909d6-f432-4422-b2dd-7ae73077e1dc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n--- Running Task 1: deploy_modelv2 ---\nRunning Python task: deploy_modelv2('power', 'aemo', 'directlake_on_onelake', 'https://raw.githubusercontent.com/djouallah/fabric_demo/refs/heads/main/semantic_model/directlake.bim')\n======================================================================\nPower BI Semantic Model Deployment\n======================================================================\n\n[Step 1/7] Getting workspace information...\n✓ Workspace ID: f24e70e8-54ec-491b-9175-2c1aef1138ce\n\n[Step 2/7] Checking if dataset 'directlake_on_onelake' exists...\n⚠️  Dataset 'directlake_on_onelake' already exists in this workspace\n\n✓ Dataset 'directlake_on_onelake' already exists - skipping deployment\n   Proceeding directly to refresh...\n\n[Step 8/9] Waiting for permission propagation...\n   Allowing time for any recent changes to propagate...\n   ⏳ 5 seconds remaining...\n✓ Wait complete\n\n[Step 9/9] Refreshing semantic model...\n   Loading data from lakehouse via DirectLake...\n✓ Successfully refreshed semantic model\n\n======================================================================\n🎉 Refresh Completed Successfully!\n======================================================================\n\nDataset Name:     directlake_on_onelake\nWorkspace ID:     f24e70e8-54ec-491b-9175-2c1aef1138ce\n\n✓ Your semantic model has been refreshed!\n======================================================================\n✅ Python task 'deploy_modelv2' completed.\n\n✅ All tasks completed successfully.\nCPU times: user 2.93 s, sys: 234 ms, total: 3.16 s\nWall time: 15.1 s\n"]},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"True"},"metadata":{}}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"9890da21-9416-48b8-b75c-fb84e9689f72"},{"cell_type":"code","source":["%%time\n","#load today data\n","con.run_pipeline(intraday)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6467316Z","session_start_time":null,"execution_start_time":"2025-10-02T11:36:12.8836402Z","execution_finish_time":"2025-10-02T11:36:22.2882094Z","parent_msg_id":"399a3e78-6d5e-4698-82c7-46c97c846e6c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n--- Running Task 1: scrapingv2 ---\nRunning Python task: scrapingv2(['http://nemweb.com.au/Reports/Current/DispatchIS_Reports/', 'http://nemweb.com.au/Reports/Current/Dispatch_SCADA/'], ['Reports/Current/DispatchIS_Reports/', 'Reports/Current/Dispatch_SCADA/'], 288, 'temp', 'power', 5)\nFlushed 3 files to disk and updated log Reports/Current/DispatchIS_Reports/download_log.csv\nFlushed 3 files to disk and updated log Reports/Current/Dispatch_SCADA/download_log.csv\nhttp://nemweb.com.au/Reports/Current/DispatchIS_Reports/ - 3 files extracted and uploaded\nhttp://nemweb.com.au/Reports/Current/Dispatch_SCADA/ - 3 files extracted and uploaded\n✅ Python task 'scrapingv2' completed.\n\n--- Running Task 2: price_today ---\n✅ SQL task 'price_today' → table 'price_today' (append) completed.\n\n--- Running Task 3: scada_today ---\n✅ SQL task 'scada_today' → table 'scada_today' (append) completed.\n\n--- Running Task 4: duid ---\nTable duid exists. Skipping (mode='ignore').\n✅ SQL task 'duid' → table 'duid' (ignore) completed.\n\n--- Running Task 5: summary__incremental ---\n✅ SQL task 'summary__incremental' → table 'summary' (append) completed.\n\n✅ All tasks completed successfully.\nCPU times: user 1.55 s, sys: 155 ms, total: 1.7 s\nWall time: 8.36 s\n"]},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"True"},"metadata":{}}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"3eed16b6-c7e1-4fb7-b85a-e7fb40c4ccf5"},{"cell_type":"markdown","source":["**_<u><mark>Files downloaded from github, it is very very slow, but free :)</mark></u>_**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"a1068e4a-aefa-45da-bc42-6f292332860e"},{"cell_type":"code","source":["%%time\n","if remaining_files > 0:\n","    ## you can turn it off when Historical data is fully loaded\n","    con.run_pipeline(history_download)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6487441Z","session_start_time":null,"execution_start_time":"2025-10-02T11:36:22.2893378Z","execution_finish_time":"2025-10-02T11:36:22.7148444Z","parent_msg_id":"538a0a21-bc80-42d4-84dd-0b9ca9ef3bc0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 5.96 µs\n"]}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"a62bd1bb-42c1-46a7-8e02-7bb1b9125b98"},{"cell_type":"code","source":["if remaining_files > 0:\n","    con.run_pipeline(history_process)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6506986Z","session_start_time":null,"execution_start_time":"2025-10-02T11:36:22.7160058Z","execution_finish_time":"2025-10-02T11:36:23.1405883Z","parent_msg_id":"f365bf9b-404b-4940-96a8-f15276f355ab"}},"metadata":{}}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"85e955e8-e5fa-4915-afd1-ec318123e2f8"},{"cell_type":"code","source":["notebookutils.notebook.exit(con.sql(\"select count(*) from summary\").fetchone()[0])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7588ef20-9261-49f2-9df3-217037251066","normalized_state":"finished","queued_time":"2025-10-02T11:35:30.6525647Z","session_start_time":null,"execution_start_time":"2025-10-02T11:36:23.14166Z","execution_finish_time":"2025-10-02T11:36:24.0353676Z","parent_msg_id":"717e0100-1655-484e-865c-53e9af9432ee"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ExitValue: 3947505"]}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"831c5a49-2ba2-4332-b900-34857bf45649"}],"metadata":{"kernel_info":{"jupyter_kernel_name":"python3.11","name":"jupyter"},"kernelspec":{"name":"jupyter","display_name":"Jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}
